{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"aca_deployment/","title":"CosmosAIGraph : Deploying the Azure Container App","text":""},{"location":"aca_deployment/#azure-container-apps","title":"Azure Container Apps","text":"<p>The recommended solution for this reference application is to deploy to Azure Container Apps (ACA).</p> <p>ACA offers a very mature and easy to use runtime environment for your applications that are packaged and deployed as Docker Containers.  Several interesting features include the following:</p> <ul> <li>Environments</li> <li>CPU and Memory sizes</li> <li>Workload Profile Types</li> <li>Networking and VNets</li> <li>Scaling Rules</li> </ul>"},{"location":"aca_deployment/#deployment-with-bicep","title":"Deployment with Bicep","text":"<p>See the impl/deployment/ directory for working deployment scripts which use the  Bicep deployment syntax.</p> <p>You can execute the az_bicep_deploy.ps1 script which uses the az CLI to deploy the ACA application. </p> <p>The Bicep file is named caig.bicep and it uses the caig.bicepparam parameters file.</p> <p>There is functionality in this repo to generate the caig.bicepparam file, and the top of your caig.bicep file, per the CAIG_xxx environment variables that you have set on your workstation.</p> <p>In the impl\\app directory you can execute the following script.</p> <pre><code>(venv) PS ...\\app&gt; python main_common.py gen_all\n\nLoggingService config level name: info\nLoggingService initialized to level: 20\n2024-03-01 16:03:21,461 - file written: tmp/caig-envvars-master.txt\n2024-03-01 16:03:21,462 - file written: ../set-caig-env-vars-sample.ps1\n2024-03-01 16:03:21,463 - file written: ../deployment/generated-param-names.bicep   &lt;--- this\n2024-03-01 16:03:21,463 - file written: ../deployment/generated.bicepparam          &lt;--- this\n2024-03-01 16:03:21,464 - file written: ../docs/environment_variables.md\n</code></pre>"},{"location":"aca_deployment/#azure-kubernetes-service-aks","title":"Azure Kubernetes Service (AKS)","text":"<p>Some customers may wish to deploy their CosmosAIGraph applications to alternative solutions, such as Azure Kubernetes Service (AKS).</p> <p>Since CosmosAIGraph is packaged as Docker Containers the use of AKS is certainly possible, but is out-of-scope for this reference application.</p>"},{"location":"application_architecture/","title":"CosmosAIGraph : Application Architecture","text":""},{"location":"application_architecture/#application-components","title":"Application Components","text":"<ul> <li>Microservices</li> <li>web microervice - UI front end with AI functionality</li> <li>graph microservice - Contains the in-memory graph</li> <li>Azure Container App - Runtime orchestrator for the above two microservices</li> <li>Cosmos DB NoSQL or Mongo vCore API - Domain data and conversational AI documents, embeddings</li> <li>Azure OpenAI - completions and embeddings service</li> </ul>"},{"location":"code_generation/","title":"CosmosAIGraph : Code Generation","text":"<p>A relatively new addition to this codebase is code generation, which currently can be used to:</p> <ul> <li>Generate an OWL ontology file</li> <li>Fully generate a standalone class RdflibTriplesBuilder, called by GraphBuilder</li> </ul> <p>Both of the above generators use metadata that can be mined from your source input data for your graph.  The format of this input data will obviously vary by customer.  It is assumed that your input data contains the relationships and attributes of your intended graph.</p>"},{"location":"code_generation/#example-metadata","title":"Example Metadata","text":"<p>This repository contains two example files, for the simple IMDb movies graph, as shown below. One file identifies the metadata for the verticies  (or entities) in your graph, while the other file identifes the edges (or relationships).</p> <pre><code>impl/data/graph_input_metadata/edge_signatures_imdb.json\nimpl/data/graph_input_metadata/vertex_signatures_imdb.json\n</code></pre>"},{"location":"code_generation/#edge-signatures-example","title":"Edge Signatures Example","text":"<pre><code>{\n  \"Movie|has_principal|Person\": 10000,\n  \"Person|in_movie|Movie\": 10000\n}\n</code></pre> <p>This metadata file captures the relationships in your data. For example, the key in the above dictionary with this value:</p> <pre><code>Movie|has_principal|Person\n</code></pre> <p>indicates that there is a relationship called has_principal that connects an instance of class Movie to a Person instance.</p> <p>The values in this metadata (i.e. - 10000) is informational only; it indicates the number of times the relationship was observed in the input data.</p>"},{"location":"code_generation/#vertex-signatures-example","title":"Vertex Signatures Example","text":"<pre><code>{\n  \"Movie|title|&lt;class 'str'&gt;\": 10000,\n  \"Movie|year|&lt;class 'int'&gt;\": 10000,\n  \"Movie|rating|&lt;class 'int'&gt;\": 10000,\n  \"Movie|genre|&lt;class 'str'&gt;\": 10000,\n  \"Person|name|&lt;class 'str'&gt;\": 10000,\n  \"Person|born|&lt;class 'int'&gt;\": 10000,\n  \"Person|died|&lt;class 'str'&gt;\": 10000\n}\n</code></pre> <p>The structure of the Vertex data is similar to the Edges data.</p> <p>For example this key: <pre><code>Person|born|&lt;class 'int'&gt;\n</code></pre></p> <p>indicates that the Person class has an born attribute, which is an integer.</p>"},{"location":"code_generation/#data-wrangling","title":"Data Wrangling","text":"<p>A simple python program can be created to read your input files, observe its structure, and produce two metadata files similarly structured to the above files.</p> <p>Since the input data for each user of CosmosAIGraph is apt to  vary widely from one application to the next, there is no standard metadata-creation python script in this reference application.</p>"},{"location":"code_generation/#execute-the-generators","title":"Execute the Generators","text":"<p>Once your metadata files exist, you can generate code as follows:</p> <pre><code>&gt; .\\venv\\Scripts\\Activate.ps1\n\n&gt; python main_console.py generate_owl meta/vertex_signatures_imdb.json meta/edge_signatures_imdb.json http://cosmosdb.com/imdb\n\n...\nfile written: tmp/generated.owl\n\n&gt; python main_console.py generate_rdflib_triples_builder meta/vertex_signatures_imdb.json\n\n...\nfile written: tmp/rdflib_triples_builder.py\n</code></pre>"},{"location":"cosmos_design_modeling/","title":"CosmosAIGraph : Cosmos DB Document Design and Modeling","text":""},{"location":"cosmos_design_modeling/#how-is-this-data-modeled-or-structured-for-graph-purposes","title":"How is this data modeled, or structured, for graph purposes?","text":"<p>One nice feature of this solution is that there is no required structure for your JSON documents in Cosmos DB.  They can be structured as if it is a non-graph application, per general NoSQL/Mongo design best practices.</p> <p>This is because the graph, in the CosmosAIGraph solution, only exists in-memory within the Python rdflib process.</p> <p>However, one suggestion is to use an edges attribute in your documents, and populate it with the list of outgoing relationships from the given entity/vertex document.</p> <p>In the case of the CosmosAIGraph reference dataset of python libraries, however, an edges attribute is not used.  Instead, the developers and dependency_ids are used at runtime to construct the in-memory RDF graph.</p> <pre><code>  ...\n  \"developers\": [\n    \"contact@palletsprojects.com\"\n  ],\n  ...\n  \"dependency_ids\": [\n    \"pypi_asgiref\",\n    \"pypi_blinker\",\n    \"pypi_click\",\n    \"pypi_importlib_metadata\",\n    \"pypi_itsdangerous\",\n    \"pypi_jinja2\",\n    \"pypi_python_dotenv\",\n    \"pypi_werkzeug\"\n  ],\n  ...\n</code></pre>"},{"location":"cosmos_design_modeling/#the-data-python-libraries-at-pypi","title":"The Data - Python Libraries at PyPi","text":"<p>The impl\\data directory in this repo contains a curated set of PyPi (Python) library JSON documents.</p> <p>This domain of software libraries was chosen because it should be relatable  to most customers, and it also suitable for Bill-of-Materials graphs.</p> <p>The PyPi JSON files were obtained with HTTP requests to public URLs such as  https://pypi.org/pypi/{libname}/json, and their HTML contents were transformed into JSON.</p> <p>Subsequent data wrangling fetched referenced HTML documentation, produced  text summarization with Azure OpenAI and semantic-kernel and produced a vectorized embedding value from several concatenated text attributes within each library JSON document.  A full description of this data wrangling process is beyond the scope of this documentation, but the process itself is in file 'impl/app/wrangle.py' in the repo.</p>"},{"location":"cosmos_design_modeling/#next-steps-load-cosmos-db-with-library-documents","title":"Next Steps: Load Cosmos DB with Library Documents","text":"<p>Depending on the Cosmos DB API you chose: - See Load Azure Cosmos DB vCore - See Load Azure Cosmos DB NoSQL</p>"},{"location":"customizing_this_solution/","title":"CosmosAIGraph : Customizing this Solution","text":"<p>This page strives to guide the reader in implementing their own application, based on CosmosAIGraph, with their own data.</p> <p>This list of steps was useful in the development of CosmosAIGraph itself, and customers are recommended to take similar steps.</p>"},{"location":"customizing_this_solution/#step-1-define-your-requirements","title":"Step 1: Define your Requirements","text":"<p>It is essential to start here.  Define your primary requirements. Specifically, define the list of graph queries in your application. Put these into a list to identify each one by a shortname (i.e. - q1, 2, etc.).</p> <p>Use of the CQRS pattern is very useful here.</p>"},{"location":"customizing_this_solution/#step-2-survey-and-scan-your-data","title":"Step 2: Survey and Scan your Data","text":"<p>Ask the question: \"Does the data that I have available enable the answering of these graph/knowledge-graph queries?\".</p> <p>If so, proceed.  If not, revisit the requirements and/or data sources.</p>"},{"location":"customizing_this_solution/#what-do-we-mean-by-scanning","title":"What do we mean by scanning?","text":"<p>By \"scanning\" we mean writing a program to read your input data to indentify its structure, attributes and their datatypes, and the relationships between the entities.  Python is an excellent choice for this scanning process, but you can use the programming language(s) of your choice.</p> <p>There are several purposes of scanning: - To ensure that it is readable and parsable - To ensure that it contains the relationships, or connections, that you expect   - The entities are linkable by specific attribute values</p> <ul> <li>To capture the metadata of your data</li> <li>Its' structure</li> <li>Attribute names and their datatypes</li> <li>Names of the classes and the relationships between them</li> <li>Accurate metadata like this can greatly accelerate your development process.<ul> <li>See Code Generation</li> </ul> </li> </ul>"},{"location":"customizing_this_solution/#step-3-understand-rdf-graph-technology-ontologies-and-sparql","title":"Step 3: Understand RDF graph technology, Ontologies, and SPARQL","text":"<p>RDF technology uses the beautifully simple concept of triples. Each triple consists of a subject, predicate, and object. A RDF database is simply an Ontology (i.e. - schema) with an array of many triples.</p> <p>Though it's an old book, this O'Reilly book was very useful in learning RDF, OWL, and SPARQL at the beginning of the development of CosmosAIGraph: https://www.oreilly.com/library/view/learning-sparql/9781449311285/</p> <p>Other learning sources include the following from the W3C:</p> <ul> <li>https://www.w3.org/RDF/</li> <li>https://www.w3.org/TR/owl2-rdf-based-semantics/</li> <li>https://www.w3.org/TR/sparql11-query/</li> </ul>"},{"location":"customizing_this_solution/#step-4-create-your-initial-web-ontology","title":"Step 4: Create your initial Web Ontology","text":"<p>Based on your requirements and data, create your own *.owl file. Define the classes and relationships pertinent to your system.</p> <p>See file impl/app/ontologies/libraries.owl as a reference.</p> <p>The owl file development process should be iterative.</p>"},{"location":"customizing_this_solution/#step-5-create-a-data-wrangling-process-to-populate-a-jena-graph","title":"Step 5: Create a Data-Wrangling process to populate a Jena graph","text":"<p>Given your datasets, and your evolving OWL file, populate  an in-memory rdflib graph, then serialize it to a *.nt file.</p> <p>Write a console application to load your graph from the OWL and nt file, and craft SPARQL queries that seek to answer the queries you identified in Step 1.</p> <p>You may need to implement programming logic to execute multiple SPARQL queries to satisfy an individual use-case requirement.</p> <p>Expect steps 4 and 5 to be iterative.  Modify your OWL, refactor your data-wrangling process, and recreate your rdflib graph and nt file.</p>"},{"location":"customizing_this_solution/#step-6-create-a-minimal-sparql-query-web-interface","title":"Step 6: Create a minimal \"SPARQL QUERY\" web interface","text":"<p>Only after you have a reasonably working graph and ontology should you try to port it to a web application.</p> <p>It is recommended that you implement your own \"SPARQL Query\" UI,  similar to the CosmosAIGraph implementation, so that you can further explore and iterate your OWL and graph.</p>"},{"location":"customizing_this_solution/#step-7-create-your-web-application-ui","title":"Step 7: Create your web application UI","text":"<p>Once your OWL and graph design is stabilized, implement your desired UI.</p> <p>This may include graph visualizations.  This reference application uses the D3.js JavaScript library for visualizations, but of course you may use any libraries you prefer.</p>"},{"location":"developer_workstation/","title":"CosmosAIGraph : Developer Workstation Setup","text":""},{"location":"developer_workstation/#required-software","title":"Required Software","text":"<p>These are required to simply execute the solution on your workstation:</p> <ul> <li>Windows 11 with PowerShell</li> <li>For Development, Windows 11 with PowerShell is recommended</li> <li>Working bash scripts are also provided for macOS users</li> <li>Git</li> <li>Distributed source control system.  Integrates with GitHub</li> <li>Enables you to git clone this GitHub repository</li> <li>Python3</li> <li>This solution uses Python command-line and web application programs, not Jupyter Notebooks</li> <li>Python version 3.12.x is recommended</li> <li>Conda is not recommended for this solution</li> <li>OpenJDK Java 21</li> <li>The new graph microservice is implemented in Java 21, Spring Boot, and Apache Jena</li> <li>Download from: https://learn.microsoft.com/en-us/java/openjdk/download#openjdk-21</li> <li>See Understanding the Code</li> <li>A Java IDE, such as:</li> <li>Eclipse</li> <li>Jetbrains Intellij</li> <li>Visual Studio Code (VSC)</li> <li>Gradle Build Tool, version 8.12</li> <li>Used to compile and package the Java code</li> <li>See https://gradle.org/</li> <li>See the build.gradle file in the repo</li> <li>Alternately use the Maven build tool<ul> <li>Create your pom.xml file per the build.gradle contents.</li> </ul> </li> <li>A Mongo Shell Program, such as:</li> <li>Azure Data Studio<ul> <li>This is the recommended mongosh program for the CosmosAIGraph solution</li> <li>See the configuration instructions here:</li> <li>https://learn.microsoft.com/en-us/azure-data-studio/quickstart-azure-cosmos-db-mongodb?tabs=mongodb-vcore</li> </ul> </li> <li>mongosh<ul> <li>The MSI installer option is recommended if you choose to use this program</li> </ul> </li> <li>Studio3T</li> </ul> <p>Also, a working knowledge of pip and Python Virtual Environments is necessary. See https://realpython.com/python-virtual-environments-a-primer/.</p> <p>This reference implementation contains several venv.ps1 and venv.sh scripts which create the Python Virtual Environments for this solution.  Once these are created, they can be activated with the following command:</p> <pre><code>&gt; .\\venv\\Scripts\\Activate.ps1\n</code></pre>"},{"location":"developer_workstation/#recommended-software","title":"Recommended Software","text":"<p>To develop your own solutions based on this reference application,  this additional software is recommended:</p> <ul> <li>Docker Desktop</li> <li>Docker Desktop will enable you to both build and execute containers</li> <li>GitHub Desktop</li> <li>Provides a nice UI and may be easier to use than the git command-line program</li> <li>Visual Studio Code (VSC)</li> <li>Lightweight IDE with multi-language support, including Python</li> <li>Integrates well with Azure, see https://code.visualstudio.com/docs/azure/extensions</li> <li>GitHub Copilot</li> <li>AI-powered coding assistant</li> <li>Copilot integrates nicely with VSC; see https://code.visualstudio.com/docs/copilot/overview</li> </ul>"},{"location":"environment_variables/","title":"CosmosAIGraph Implementation 1 : Environment Variables","text":"<p>Per the Twelve-Factor App methodology, configuration is stored in environment variables. This is the standard practice for Docker-containerized applications deployed to orchestrators such as Azure Kubernetes Service (AKS) and Azure Container Apps (ACA).</p>"},{"location":"environment_variables/#defined-variables","title":"Defined Variables","text":"<p>This reference implementation uses the following environment variables. All of these begin with the prefix <code>CAIG_</code>.</p> Name Description Where Used CAIG_AZURE_OPENAI_COMPLETIONS_DEP The name of your Azure OpenAI completions deployment. WEB RUNTIME CAIG_AZURE_OPENAI_EMBEDDINGS_DEP The name of your Azure OpenAI embeddings deployment. WEB RUNTIME CAIG_AZURE_OPENAI_KEY The Key of your Azure OpenAI account. WEB RUNTIME CAIG_AZURE_OPENAI_URL The URL of your Azure OpenAI account. WEB RUNTIME CAIG_CONFIG_CONTAINER The Cosmos DB container for configuration JSON values. RUNTIME CAIG_CONVERSATIONS_CONTAINER The Cosmos DB container where the chat conversations and history are persisted. WEB RUNTIME CAIG_COSMOSDB_NOSQL_ACCT The Name of your Cosmos DB NoSQL account. RUNTIME CAIG_COSMOSDB_NOSQL_AUTH_MECHANISM The Cosmos DB NoSQL authentication mechanism; key or rbac. RUNTIME CAIG_COSMOSDB_NOSQL_KEY The key of your Cosmos DB NoSQL account. RUNTIME CAIG_COSMOSDB_NOSQL_RG The Resource Group of your Cosmos DB NoSQL account. DEV ENV CAIG_COSMOSDB_NOSQL_URI The URI of your Cosmos DB NoSQL account. RUNTIME CAIG_FEEDBACK_CONTAINER The Cosmos DB container where user feedback is persisted. WEB RUNTIME CAIG_GRAPH_DUMP_OUTFILE The file to write to if CAIG_GRAPH_DUMP_UPON_BUILD is true. GRAPH RUNTIME CAIG_GRAPH_DUMP_UPON_BUILD Boolean true/false to dump the Java/Jena model to CAIG_GRAPH_DUMP_OUTFILE. GRAPH RUNTIME CAIG_GRAPH_NAMESPACE The custom namespace for the RED graph. GRAPH RUNTIME CAIG_GRAPH_SERVICE_NAME Logical app name. DEV ENV CAIG_GRAPH_SERVICE_PORT 8002 WEB RUNTIME CAIG_GRAPH_SERVICE_URL http://127.0.0.1 or determined by ACA. WEB RUNTIME CAIG_GRAPH_SOURCE_CONTAINER The graph Cosmos DB container name, if CAIG_GRAPH_SOURCE_TYPE is 'cosmos_nosql'. GRAPH RUNTIME CAIG_GRAPH_SOURCE_DB The graph Cosmos DB database name, if CAIG_GRAPH_SOURCE_TYPE is 'cosmos_nosql'. GRAPH RUNTIME CAIG_GRAPH_SOURCE_OWL_FILENAME The input RDF OWL ontology file. GRAPH RUNTIME CAIG_GRAPH_SOURCE_RDF_FILENAME The RDF input file, if CAIG_GRAPH_SOURCE_TYPE is 'rdf_file'. GRAPH RUNTIME CAIG_GRAPH_SOURCE_TYPE The RDF graph data source type, either 'cosmos_nosql', or 'json_docs_file' or 'rdf_file'. GRAPH RUNTIME CAIG_HOME Root directory of the CosmosAIGraph GitHub repository on your system. DEV ENV CAIG_LOG_LEVEL A standard python or java logging level name. RUNTIME CAIG_WEBSVC_AUTH_HEADER Name of the custom HTTP authentication header; defaults to 'x-caig-auth'. RUNTIME CAIG_WEBSVC_AUTH_VALUE your-secret-value RUNTIME CAIG_WEB_APP_NAME Logical name. DEV ENV CAIG_WEB_APP_PORT 8000 WEB RUNTIME CAIG_WEB_APP_URL http://127.0.0.1 or determined by ACA. WEB RUNTIME"},{"location":"environment_variables/#setting-these-environment-variables","title":"Setting these Environment Variables","text":"<p>The repo contains generated PowerShell script impl/set-caig-env-vars-sample.ps1 which sets all of these CAIG_ environment values. You may find it useful to edit and execute this script rather than set them manually on your system</p>"},{"location":"environment_variables/#python-dotenv","title":"python-dotenv","text":"<p>The python-dotenv library is used in each subapplication of this implementation. It allows you to define environment variables in a file named <code>.env</code> and thus can make it easier to use this project during local development.</p> <p>Please see the dotenv_example files in each subapplication for examples.</p> <p>It is important for you to have a .gitignore entry for the .env file so that application secrets don't get leaked into your source control system.</p>"},{"location":"environment_variables/#java-overrideproperties-file","title":"Java .override.properties file","text":"<p>The Java codebase in this repo implements similar logic to the python-dotenv described above.</p> <p>See file example-override.properties in the impl/graph_app/ directory.</p>"},{"location":"faq/","title":"CosmosAIGraph : FAQ","text":"<p>This page addresses some of the Frequently Asked Questions about the CosmosAIGraph solution.</p>"},{"location":"faq/#list-of-questions","title":"List of Questions","text":"<ul> <li>Is CosmosAIGraph a supported Microsoft product?</li> <li>Is CosmosAIGraph the same as the Microsoft Research GraphRAG whitepaper?</li> <li>Should I store my Docker images in DockerHub, too?</li> <li>Do I have to use the Python programming language?</li> <li>Does the in-memory graph scale to huge datasets?</li> <li>Can the Graph be changed once it is loaded into memory?</li> <li>Do I have to use Azure Container Apps (ACA)?</li> <li>So, CosmosAIGraph is a \"framework\" like Spring Boot that I simply plug into?</li> <li>I'm just learning about Ontologies and OWL.  How should I create my graph schema?</li> <li>What are RDF Triples?</li> <li>Can I add additional data sources with the OmniRAG pattern, and how?</li> <li>Do I have to use D3.js for web page graph visualizations?</li> <li>Should I store PDFs, images, and movies in Cosmos DB?</li> <li>How should I load my data into Cosmos DB?</li> <li>I don't understand the Bicep parameter names, can you explain?</li> </ul>"},{"location":"faq/#answers","title":"Answers","text":"<p>Q: Is CosmosAIGraph a supported Microsoft product?</p> <p>A: It is a reference implementation rather than a product. Customers are free to use it as-is, or modify it for their needs.</p> <p>The Cosmos DB Global Black Belt team created the solution and support customers in its use.  We integrate customer feedback and field experience into this evolving solution.</p> <p></p> <p>Q: Is CosmosAIGraph the same as the Microsoft Research GraphRAG whitepaper?</p> <p>A: No.  This is a separate project, not related the MSR GraphRAG concept. See https://www.microsoft.com/en-us/research/blog/graphrag-unlocking-llm-discovery-on-narrative-private-data/?msockid=1f6ce974c2e161512deefd31c3b760f3</p> <p>The CosmosAIGraph solution differs in three significant ways:</p> <ul> <li>It implements the OmniRAG pattern, for n-number of RAG data sources.</li> <li>It uses Microsoft Databases (i.e. - Cosmos DB)</li> <li>It is supported by the Global Black Belt team during customer engagements.</li> </ul> <p></p> <p>Q: Should I store my Docker images in DockerHub, too?</p> <p>A: No.  It is recommended that you use  Azure Container Registry (ACR) for your images for security purposes.</p> <p>We use DockerHub for this public project, with public data,  so that all customers can access it.</p> <p></p> <p>Q: Do I have to use the Python programming language?</p> <p>A: For the Graph Microservice, yes, since the implementation uses the rdflib Python library.</p> <p>However, for the Web Application you can use any programming language you'd like (i.e. - Java, C#, Node.js, etc.) as long as it can make HTTP calls to the Graph Microservice.</p> <p>Some customers only deploy the Graph Microservice and integrate it, via HTTP calls, into their existing UI applications.</p> <p></p> <p>Q: Does the in-memory graph scale to huge datasets?</p> <p>A: The idea is to store the bare minimum of data in the in-memory graph that is sufficient for your graph query use-cases.  This is usually a small subset of the attributes in your Cosmos DB documents.</p> <p>We've tested an in-memory graph with over 20 million triples, but much larger graphs are possible.</p> <p>Azure Container Apps (ACA) workload profiles support up to 880 GB of memory, see https://learn.microsoft.com/en-us/azure/container-apps/workload-profiles-overview.  This enables huge in-memory graphs.  </p> <p>Also, we recommend at least two instances of the graph service in ACA for high-availibility.  Example Bicep configuration shown below.</p> <pre><code>      scale: {\n        maxReplicas: 2\n        minReplicas: 2\n      }\n</code></pre> <p></p> <p>Q: Can the Graph be changed once it is loaded into memory?</p> <p>A: Yes, though the reference implementation doesn't demonstrate this.</p> <p>One way to implement this is to use the Cosmos DB NoSQL API Change Feed functionality to observe changes to the database, enqueue these, and have the Graph Microservice(s) process these queued changes.</p> <p>Alternatively, the Cosmos DB vCore API offers change-stream functionality (currently in preview mode).</p> <p></p> <p>Q: Do I have to use Azure Container Apps (ACA)?</p> <p>A: No.  Azure Kubernetes Service (AKS) is also recommended.</p> <p></p> <p>Q: So, CosmosAIGraph is a \"framework\" like Spring Boot that I simply plug into?</p> <p>A: No.  The CosmosAIGraph codebase is a reference implementation rather than a framework.  It provides a working example and software patterns that you may wish to use in your application.</p> <p>It is expected that each customer will significantly modify the codebase for their particular needs.</p> <p>We recommend that customer skillsets include application programming with Python, Web UI skills, and AI/LLM/Prompt skills. While PySpark experience is useful, this solution is not based on Spark.</p> <p></p> <p>Q: I'm just learning about Ontologies and OWL.  How should I create my graph schema?</p> <p>A: There are several ways to do this, and in our opinion it's easier to do than model relational databases.</p> <p>One way is to simply create a Visio or similar diagram of the vertices and edges of your graph, then use this visual model to manually author the corresponding OWL XML syntax.  This is our recommended approach.</p> <p>Please see the item below on What are RDF Triples?.  We use the terms  vertices and edges here conceptually, but not for implementation.</p> <p>Another way is to write a Python or other program to read and scan your input data, usually before loading it into Cosmos DB.  The program identifies all of the entity types, their attribute names and datatypes, and the relationships to other entities.  This extracted \"metadata\" of your data can then be used to generate your OWL file.  This approach has been successfully used with several customers. As an additional bonus, the metadata can also be used to generate Python code, such as the logic that reads Cosmos DB and populates the in-memory graph.</p> <p>Yet another way is to leverage Generative AI to scan the data and generate the OWL ontology.  This may be a more expensive and time-consuming process,  however.</p> <p>We recommend these two books on RDF, SPARQL, and OWL to accelerate your learning: - https://www.oreilly.com/library/view/learning-sparql/9781449311285/ - https://www.oreilly.com/library/view/practical-rdf/0596002637/</p> <p></p> <p>Q: What are RDF Triples?</p> <p>A: RDF graph databases use \"triples\" of (subject, predicate, object) to implement the graph.</p> <p>We used the terms vertices and edges above as concepts to help you visualize the graph, but RDF graph databases don't actually use or implement vertices and edges. Vertices and edges are concepts used in LPG graph databases, such as Neo4j and Cosmos DB Gremlin.</p> <p>RDF instead uses \"triples\" to implement the graph. These consist of a tuple of (subject, predicate, object).</p> <p>For example, given these two conceptual triples:</p> <pre><code>Aleksey --&gt; Works At --&gt; Microsoft\nChris   --&gt; Works At --&gt; Microsoft\n</code></pre> <p>The RDF graph is then able to infer the relationship, or edge, between Chris and Aleksey.  In our experience, this is a significant advantage  of RDF over LPG, as you don't need to explicitly create the edges in your graph.  We have seen many LPG graph project with sub-optimal datasets as, over time, vertices get programatically deleted but their corresponding edges often do not.  This leads to orphan edges that can greatly reduce database performance.</p> <p>The following are the actual triples in the reference implementation graph relating to the m26 python library.  Each line represents a triple. The subjects, predicates, and objects are usually expressed as URI values that correspond to your graph ontology, or OWL file.</p> <p>Though these verbose values look inefficient, within the database they become efficient data structures.</p> <pre><code>&lt;http://cosmosdb.com/caig/pypi_m26&gt; &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#type&gt; &lt;http://cosmosdb.com/caig#Lib&gt; .\n&lt;http://cosmosdb.com/caig/pypi_m26&gt; &lt;http://cosmosdb.com/caig#lic&gt; \"mit\" .\n&lt;http://cosmosdb.com/caig/christopher.joakim@gmail.com&gt; &lt;http://cosmosdb.com/caig#developer_of&gt; &lt;http://cosmosdb.com/caig/pypi_m26&gt; .\n&lt;http://cosmosdb.com/caig/christopher_joakim&gt; &lt;http://cosmosdb.com/caig#developer_of&gt; &lt;http://cosmosdb.com/caig/pypi_m26&gt; .\n&lt;http://cosmosdb.com/caig/pypi_m26&gt; &lt;http://cosmosdb.com/caig#developed_by&gt; &lt;http://cosmosdb.com/caig/christopher.joakim@gmail.com&gt; .\n&lt;http://cosmosdb.com/caig/pypi_m26&gt; &lt;http://cosmosdb.com/caig#ln&gt; \"m26\" .\n&lt;http://cosmosdb.com/caig/pypi_m26&gt; &lt;http://cosmosdb.com/caig#developed_by&gt; &lt;http://cosmosdb.com/caig/christopher_joakim&gt; .\n&lt;http://cosmosdb.com/caig/pypi_m26&gt; &lt;http://cosmosdb.com/caig#kwds&gt; \"pace_per_mile runwalkcalculator seconds_per_mile agecalculator calculates\" .\n&lt;http://cosmosdb.com/caig/pypi_m26&gt; &lt;http://cosmosdb.com/caig#lt&gt; \"pypi\" .\n</code></pre> <p>See Wikipedia for more information on Semantic Triples</p> <p></p> <p>Q: Can I add additional data sources with the OmniRAG pattern, and how?</p> <p>A: Yes.  You are free to customize the code per your needs.</p> <p>We recommend sourcing the in-memory graph from Cosmos DB, but you  are free to use other sources.</p> <p>Class StrategyBuilder determines the \"strategy\" to use regarding where to search for the appropriate RAG data given a user utterance. This logic, as well as the corresponding LLM prompt, will need to be modified.</p> <p></p> <p>Q: Do I have to use D3.js for web page graph visualizations?</p> <p>A: No.  You are free to use your preferred JavaScript visualization library.  We chose D3.js because it is free, widely used, and is generally of high quality.</p> <p></p> <p>Q: Should I store PDFs, images, and movies in Cosmos DB?</p> <p>A: No.  Use Cosmos DB to store your documents as JSON objects. Typical document size is 1-100kb, though larger documents are supported. The word \"document\" is overloaded in IT.  In the context of Cosmos DB, a \"document\" means \"JSON object\".</p> <p>Binary content such as PDFs, images, and movies should be stored in Azure Blob Storage or data lake, but their \"metadata\" (i.e - filename, type, location, description, summary, etc.) can be stored in Cosmos DB as JSON objects.</p> <p></p> <p>Q: How should I load my data into Cosmos DB?</p> <p>A: There are many ways to do this, including:</p> <ul> <li>Azure Data Factory</li> <li>Spark (in Azure Synapse or Microsoft Fabric) with Cosmos DB connector library</li> <li>Programming language SDK</li> <li>The Java and C# APIs for the Cosmos DB NoSQL API offer excellent \"bulk loading\" functionality.</li> </ul> <p></p> <p>Q: I don't understand the Bicep parameter names, can you explain?</p> <p>A: The bicep parameter names are camel-cased versions of the environment variable names that are described in class ConfigService.  For example, the bicep name \"graphSourceDb\" maps to environment variable \"CAIG_GRAPH_SOURCE_DB\".</p> <p>Please see main_common.py which implements the following command-line functions which you can use to generate Bicep and Compose file fragments from the list of your defined environment variables in class ConfigService.</p> <pre><code>    python main_common.py gen_ps1_env_var_script\n    python main_common.py gen_bicep_file_fragments\n    python main_common.py gen_environment_variables_md\n    python main_common.py gen_all\n</code></pre>"},{"location":"fastapi_endpoint_docs/","title":"CosmosAIGraph : FastAPI Framework and Endpoint Documentation","text":""},{"location":"fastapi_endpoint_docs/#fastapi","title":"FastAPI","text":"<p>This CosmosAIGraph reference implementation is entirely written in Python3 and all microservices utilize the FastAPI web framework.</p> <p>FastAPI is a modern and asynchronous web framework, and is seen as the successor to Flask.  It is also greatly simpler than the Django web framework, which is oriented toward RDBMS-backed web applications.</p>"},{"location":"fastapi_endpoint_docs/#automated-endpoint-documentation","title":"Automated Endpoint Documentation","text":"<p>With FastAPI, you can leverage the Pydantic library to define your endpoint requests and responses as Pydantic Models as described here: https://fastapi.tiangolo.com/tutorial/body-nested-models/</p> <p>This can enable automatic generation of OpenAPI/Swagger documentation that is useful to the producers and consumers of your microservices.</p> <p>You can invoke the FastAPI-created /docs endpoint to see the generated documentation.  An example is shown in the screen shot below.</p> <p> </p> <p>This functionality can be disabled as described here: https://fastapi.tiangolo.com/tutorial/metadata/#docs-urls</p>"},{"location":"initial_paas_provisioning/","title":"CosmosAIGraph : Initial PaaS Provisioning","text":"<p>Though the provisioning of Azure PaaS services can be fully automated, it is recommended that you deploy Azure OpenAI and Azure Cosmos DB Mongo vCore manually in your subscription for this reference application.</p> <p>The reason for this is that you may already have these resources deployed (i.e - Azure OpenAI) and they may not be available in all regions.  Furthermore, you may face Azure OpenAI model quota constraints.</p>"},{"location":"initial_paas_provisioning/#azure-openai","title":"Azure OpenAI","text":"<p>In Azure Portal, go to Marketplace and search for \"Azure OpenAI\". Proceed through the dialogs to provision this resource.</p> <p>Once it is provisioned, navigate to the Resource Management -&gt; Keys and Endpoint page within the account as shown below.</p> <p> </p> <p>Capture the values of the endpoint URL and Key 1.  You will later set your CAIG_AZURE_OPENAI_URL and CAIG_AZURE_OPENAI_KEY environment variables with these values.</p> <p>Create model deployments for the gpt-4 and text-embedding-ada-002 models. It is expected that, over time, the names of these models will change, so use the current version of each.</p> <p> </p> <p>You should set your CAIG_AZURE_OPENAI_COMPLETIONS_DEP and  CAIG_AZURE_OPENAI_EMBEDDINGS_DEP environment variables to the deployment names of these two models in your Azure OpenAI account.</p>"},{"location":"initial_paas_provisioning/#azure-container-registry-acr","title":"Azure Container Registry (ACR)","text":"<p>See https://learn.microsoft.com/en-us/azure/container-registry/.</p> <p>When you build the Docker images for your application they should be pushed to your ACR.</p> <p>The example Bicep deployment scripts in this repository use DockerHub for this public reference implementation. However, you should use your private and secure Azure Container Registry instead.</p>"},{"location":"initial_paas_provisioning/#azure-cosmos-db-mongo-vcore","title":"Azure Cosmos DB Mongo vCore","text":"<p>In Azure Portal, go to Marketplace and search for \"Azure Cosmos DB\". Select \"Azure Cosmos DB for Mongo DB\" on the first page:</p> <p> </p> <p>Then select \"vCore Cluster\" on the second page as shown below.</p> <p> </p> <p>Choose the Cluster Tier of your choice.  The Free Tier or M25 Tier will be adequate for this reference application.</p> <p>Be sure to record the Admin Username and Password values that you provide. These values will be needed to form the connection string.</p>"},{"location":"initial_paas_provisioning/#obtainset-the-connection-string","title":"Obtain/Set the Connection String","text":"<p>After the Azure Cosmos DB Mongo vCore account has been created, navigate to it in your Azure Portal and navigate to the \"Settings -&gt; Connection Strings\" page as shown below.  </p> <p> </p> <p>Copy the \"Connection string\" value on that page into a text editor;  it should look like the following.  Edit the part of the connection string with the content \"user:password\" and substitute the  Admin Username and Password that you specified above when creating the account.</p>"},{"location":"initial_paas_provisioning/#raw-connection-string-from-azure-portal","title":"Raw Connection String from Azure Portal","text":"<pre><code>mongodb+srv://&lt;user&gt;:&lt;password&gt;@&lt;your-account-name&gt;.mongocluster.cosmos.azure.com/?tls=true&amp;authMechanism=SCRAM-SHA-256&amp;retrywrites=false&amp;maxIdleTimeMS=120000\n</code></pre>"},{"location":"initial_paas_provisioning/#sample-edited-connection-string-from-azure-portal","title":"Sample Edited Connection String from Azure Portal","text":"<pre><code>mongodb+srv://chris:secret@&lt;your-account-name&gt;.mongocluster.cosmos.azure.com/?tls=true&amp;authMechanism=SCRAM-SHA-256&amp;retrywrites=false&amp;maxIdleTimeMS=120000\n</code></pre> <p>Save this edited connection string value.  You will later set your CAIG_AZURE_MONGO_VCORE_CONN_STR environment variable with this value.</p>"},{"location":"load_cosmos_nosql/","title":"CosmosAIGraph : Load Azure Cosmos DB for NoSQL","text":""},{"location":"load_cosmos_nosql/#configuration","title":"Configuration","text":"<p>This page assumes that you have set the following environment variables:</p> <pre><code>CAIG_GRAPH_SOURCE_TYPE              &lt;-- must be set to 'cosmos_nosql'\nCAIG_COSMOSDB_NOSQL_URI             &lt;-- this value is unique to your Azure deployment\nCAIG_COSMOSDB_NOSQL_KEY1            &lt;-- Read/Write key value\nCAIG_COSMOSDB_NOSQL_AUTH_MECHANISM  &lt;-- Authentication mechanism - key or RBAC (Entra ID)\n\nCAIG_GRAPH_SOURCE_DB                &lt;-- defaults to 'caig'\nCAIG_GRAPH_SOURCE_CONTAINER         &lt;-- defaults to 'libraries'\nCAIG_CONFIG_CONTAINER               &lt;-- defaults to 'config'\nCAIG_CONVERSATIONS_CONTAINER        &lt;-- defaults to 'conversations'\nCAIG_FEEDBACK_CONTAINER             &lt;-- defaults to 'feedback'\n</code></pre>"},{"location":"load_cosmos_nosql/#create-the-cosmos-db-nosql-containers","title":"Create the Cosmos DB NoSQL Containers","text":"<p>Use Azure Portal to create the caig database and the several containers within it, listed below.  This can later be automated with your provisioning toolset of choice - Bicep, ARM, Terraform, etc..</p>"},{"location":"load_cosmos_nosql/#the-libraries-container","title":"The Libraries Container","text":"<p>First create the libraries container, with a diskANN vector index, as shown in this screen shot.  This is an example of a domain data container that contains your vectorized \"business data\".</p> <p> </p>"},{"location":"load_cosmos_nosql/#the-other-containers","title":"The Other Containers","text":"<p>Also manually create these three containers in the caig database.</p> Container Partition Key Attribute Request Units config /pk 4000 autoscale conversations /pk 10000 autoscale feedback /conversation_id 4000 autoscale <p>The config container will contain one document which identifies the entities in this system.  This is used by classes EntitiesService and StrategyBuilder to analyze user-specified natural-language queries.</p> <p>The conversations and feedback containers record the evolving state of AIConversations.  These containers will be useful for  performing analysis and optimization of your AI logic - models, prompts, etc.. Mirroring this data to OneLake in Microsoft Fabric may be a cost-effective solution.</p> <p>Navigate to the impl\\app directory of this repo and execute the following commands:</p> <pre><code>&gt; .\\venv.ps1                      &lt;-- create the python virtual environment\n\n&gt; .\\venv\\Scripts\\Activate.ps1     &lt;-- activate the python virtual environment\n</code></pre>"},{"location":"load_cosmos_nosql/#load-the-entities-document-into-the-config-container","title":"Load the entities document into the config container","text":"<p>This step will load one document into the config container. The document contains a list of the known entities in the system.</p> <pre><code>&gt; python main_nosql.py load_entities caig config\n</code></pre> <p>The input file/document is impl/data/entities/entities_doc.json,  which looks like the following:</p> <p>{   \"id\": \"entities\",   \"pk\": \"entities\",   \"created_at\": 1724621802.222312,   \"created_date\": \"2024-08-25 17:36:42.222312\",   \"docs_read\": 10855,   \"elapsed_seconds\": \"0.454175900\",   \"exception\": \"\",   \"libraries\": {     \"aiotask-context\": \"pypi\",     \"pyxirr\": \"pypi\",     \"price-parser\": \"pypi\",     \"rl-renderpm\": \"pypi\",     \"mypy-boto3-lexv2-models\": \"pypi\",     ... many other libraries ...   } }</p> <p>This list of entities is used in class EntitiesService and StrategyBuilder. For your CosmosAIGraph implementation, create and upload a similar file.</p>"},{"location":"load_cosmos_nosql/#load-the-library-data-into-cosmos-db-nosql","title":"Load the Library data into Cosmos DB NoSQL","text":"<p>This step will load the main dataset into a libraries container :</p> <pre><code>&gt; python main_nosql.py load_libraries caig libraries 999999\n\n2024-09-05 12:27:28,239 - load_libraries, dbname: caig, cname: libraries, max_docs: 999999\n2024-09-05 12:27:28,239 - CosmosNoSQLService - constructor\n2024-09-05 12:27:28,282 - CosmosNoSQLService - initialize() completed\n2024-09-05 12:27:28,724 - reading file 0 of 10854: ../data/pypi/wrangled_libs/2captcha-python.json\n2024-09-05 12:27:28,744 - reading file 1 of 10854: ../data/pypi/wrangled_libs/2to3.json\n2024-09-05 12:27:28,759 - reading file 2 of 10854: ../data/pypi/wrangled_libs/a2wsgi.json\n2024-09-05 12:27:28,774 - reading file 3 of 10854: ../data/pypi/wrangled_libs/aadict.json\n2024-09-05 12:27:28,782 - reading file 4 of 10854: ../data/pypi/wrangled_libs/aafigure.json\n2024-09-05 12:27:28,797 - reading file 5 of 10854: ../data/pypi/wrangled_libs/ablog.json\n2024-09-05 12:27:28,812 - reading file 6 of 10854: ../data/pypi/wrangled_libs/about-time.json\n2024-09-05 12:27:28,830 - reading file 7 of 10854: ../data/pypi/wrangled_libs/aboutcode-toolkit.json\n2024-09-05 12:27:28,852 - reading file 8 of 10854: ../data/pypi/wrangled_libs/absl-py.json\n2024-09-05 12:27:28,872 - reading file 9 of 10854: ../data/pypi/wrangled_libs/acachecontrol.json\n2024-09-05 12:27:30,290 - load_batch 1 with 10 documents, results: {\"201\": 10}\n2024-09-05 12:27:30,290 - current totals: {\"document_files_read\": 10, \"201\": 10}\n... many batches of 10 documents each loaded ...\n</code></pre>"},{"location":"load_cosmos_nosql/#execute-a-vector-search-with-the-loaded-data","title":"Execute a Vector Search with the loaded data","text":"<p>First generate an embedding value from the words: \"asynchronous web framework with pydantic\". Then use that embedding in a vector search vs the Cosmos DB libraries container.</p> <pre><code>&gt; python main_nosql.py vector_search_words asynchronous web framework with pydantic\n\n...\ndoc 0: {'pk': 'pypi', 'id': 'pypi_fastapi', 'name': 'fastapi', 'libtype': 'pypi', 'score': 0.817302858227684}\ndoc 1: {'pk': 'pypi', 'id': 'pypi_flask_pydantic', 'name': 'flask-pydantic', 'libtype': 'pypi', 'score': 0.8144557229062142}\ndoc 2: {'pk': 'pypi', 'id': 'pypi_falcon', 'name': 'falcon', 'libtype': 'pypi', 'score': 0.8098353146347737}\ndoc 3: {'pk': 'pypi', 'id': 'pypi_async_asgi_testclient', 'name': 'async-asgi-testclient', 'libtype': 'pypi', 'score': 0.8090871851235626}\n...\n</code></pre> <p>Notice that the FastAPI library is correctly identified as the top semantic search result.</p>"},{"location":"load_cosmos_vcore/","title":"CosmosAIGraph : Load Azure Cosmos DB vCore","text":""},{"location":"load_cosmos_vcore/#configuration","title":"Configuration","text":"<p>This page assumes that you have set the following environment variables:</p> <pre><code>CAIG_GRAPH_SOURCE_TYPE              &lt;-- must be set to 'cosmos_vcore'\nCAIG_AZURE_MONGO_VCORE_CONN_STR     &lt;-- this value is unique to your Azure deployment\n\nCAIG_GRAPH_SOURCE_DB                &lt;-- defaults to 'caig'\nCAIG_GRAPH_SOURCE_CONTAINER         &lt;-- defaults to 'libraries'\nCAIG_CONFIG_CONTAINER               &lt;-- defaults to 'config'\nCAIG_CONVERSATIONS_CONTAINER        &lt;-- defaults to 'conversations'\nCAIG_FEEDBACK_CONTAINER             &lt;-- defaults to 'feedback'\n</code></pre>"},{"location":"load_cosmos_vcore/#create-the-cosmos-db-mongo-vcore-collections-and-indexes","title":"Create the Cosmos DB Mongo vCore Collections and Indexes","text":"<p>Navigate to the impl\\app directory of this repo and execute the following commands:</p> <pre><code>&gt; .\\venv.ps1                      &lt;-- create the python virtual environment\n\n&gt; .\\venv\\Scripts\\Activate.ps1     &lt;-- activate the python virtual environment\n\n&gt; python main_vcore.py create_vcore_collections_and_indexes\n</code></pre>"},{"location":"load_cosmos_vcore/#manually-create-the-vector-search-index","title":"Manually create the Vector Search Index","text":"<p>See https://learn.microsoft.com/en-us/azure/cosmos-db/mongodb/vcore/vector-search for the documentation on this index type.</p> <p>In your mongo shell program excute the following command in your database.</p> <pre><code>use caig\n\ndb.runCommand({\n  createIndexes: 'libraries',\n  indexes: [\n    {\n      name: 'vectorSearchIndex',\n      key: {\n        \"embedding\": \"cosmosSearch\"\n      },\n      cosmosSearchOptions: {\n        kind: 'vector-ivf',\n        numLists: 20,\n        similarity: 'COS',\n        dimensions: 1536\n      }\n    }\n  ]\n});\n</code></pre>"},{"location":"load_cosmos_vcore/#verify-the-indexes-several-containers","title":"Verify the Indexes several Containers","text":"<p>First, get the list of containers.  These four container names should be present.</p> <pre><code>[mongos] caig&gt; db.getCollectionNames()\n[ 'libraries', 'cache', 'config', 'conversations' ]\n</code></pre> <p>Next, run the following getIndexes() command for each container to  confirm that the necessary indexes have been created.  They should look like the following.</p> <pre><code>db.cache.getIndexes()\n\n[\n  { v: 2, key: { _id: 1 }, name: '_id_' },\n  { v: 2, key: { cache_key: 1 }, name: 'cache_key_1' }\n]\n</code></pre> <pre><code>db.config.getIndexes()\n\n[\n  { v: 2, key: { _id: 1 }, name: '_id_' },\n  { v: 2, key: { id: 1 }, name: 'id_1' }\n]\n</code></pre> <pre><code>db.conversations.getIndexes()\n\n[\n  { v: 2, key: { _id: 1 }, name: '_id_' },\n  { v: 2, key: { conversation_id: 1 }, name: 'conversation_id_1' },\n  { v: 2, key: { created_date: 1 }, name: 'created_date_1' },\n  { v: 2, key: { created_at: 1 }, name: 'created_at_1' }\n]\n</code></pre> <pre><code>db.libraries.getIndexes()\n\n[\n  { v: 2, key: { _id: 1 }, name: '_id_' },\n  { v: 2, key: { id: 1 }, name: 'id_1' },\n  { v: 2, key: { name: 1 }, name: 'name_1' },\n  { v: 2, key: { libtype: 1 }, name: 'libtype_1' },\n  {\n    v: 2,\n    key: { embedding: 'cosmosSearch' },\n    name: 'vectorSearchIndex',\n    cosmosSearchOptions: {\n      kind: 'vector-ivf',\n      numLists: 20,\n      similarity: 'COS',\n      dimensions: 1536\n    }\n  }\n]\n</code></pre>"},{"location":"load_cosmos_vcore/#load-the-library-data-into-cosmos-db-mongo-vcore","title":"Load the Library data into Cosmos DB Mongo vCore","text":"<p>Navigate to the impl\\app directory of this repo and execute the following commands:</p> <pre><code>&gt; .\\venv\\Scripts\\Activate.ps1     &lt;-- activate the python virtual environment\n\n&gt; python main_vcore.py load_vcore_with_library_documents\n\n&gt; python main_vcore.py persist_entities\n</code></pre> <p>The load_vcore_with_library_documents function loads the libraries container, while the persist_entities function loads one document into the config container.</p>"},{"location":"load_cosmos_vcore/#verifying-the-data-load","title":"Verifying the Data Load","text":"<p>In your mongosh program, execute the following commands:</p>"},{"location":"load_cosmos_vcore/#document-queries","title":"Document Queries","text":"<pre><code>db.libraries.findOne()\n... you'll see a random document from the libraries container here ...\n\ndb.libraries.find({name: \"flask\"})\n... see the flask document ...\n</code></pre> <p>The JSON output of the above flask query should look identical to file impl/data/pypi/wrangled_libs/flask.json in this repo.</p>"},{"location":"load_cosmos_vcore/#document-counts","title":"Document Counts","text":"<pre><code>db.libraries.countDocuments()\n10855\n\ndb.config.countDocuments()\n1\n</code></pre> <p>10855 is the expected number of documents in the libraries container, while one document is expected in the config container.</p> <p>The number 10855 corresponds to the number of JSON documents in directory  'impl/data/pypi/wrangled_libs/' in this repo.</p>"},{"location":"local_execution/","title":"CosmosAIGraph : Local Execution","text":"<p>This page is oriented toward Software Engineers who want to explore and execute this application on their workstation.</p> <p>Other users may simply wish to deploy the pre-built Docker containers to an Azure Container App as described here.</p>"},{"location":"local_execution/#unzip-file-libraries-graphzip","title":"Unzip file libraries-graph.zip","text":"<p>The full-size libraries-graph.nt (N-triples) file is too large for GitHub. Therefore it has been zipped and added to this repo as a file  impl/app/rdf/libraries-graph.zip.  Navigate to this directory and unzip this file to impl/app/rdf/libraries-graph.nt .</p>"},{"location":"local_execution/#modes-of-execution","title":"Modes of Execution","text":"<p>Three different modes of execution are recommended, please use the mode most natural to your development style.</p> <p>The three modes are:</p> <ul> <li>Microservice-per-Terminal</li> <li>Launcher Script</li> <li>Docker Compose</li> </ul>"},{"location":"local_execution/#microservice-per-terminal","title":"Microservice-per-Terminal","text":"<p>In this mode, you two Terminal windows are created each hosting its microservice, you can do that either by navigate to impl/app/ in each, creating/activating the python virtual environment, and starting the webapp.ps1 or websvc.ps1 script or simply running impl/run.ps1 script.</p> <p>Be sure to set your environment variables, either by preparing and .env file, or by preparing and running impl/set-caig-env-vars-sample.ps1 script, before starting the microservices.</p>"},{"location":"local_execution/#docker-compose","title":"Docker Compose","text":"<p>This is the closest method to running your application to Azure. This approach executes the application packaged as Docker Containers rather than as local files.</p> <p>Start your Docker Desktop application if it's not already running.</p> <p>Be sure to modify your environment variables in the appropriate docker-compose-xxx.yml ile before starting the microservices.</p> <p>Two docker-compose yml files are available:</p> <ul> <li>docker/docker-compose-with-rdflib.yml</li> <li>This uses the Python-based web application</li> <li> <p>This uses the Python-based graph microservice using rdflib</p> </li> <li> <p>docker-compose-with-jena.yml </p> </li> <li>This also uses the same Python-based web application</li> <li>This uses the Java-based graph microservice using Apache Jena</li> </ul> <p>Create two PowerShell Terminal windows, and navigate to the impl/app/ directory in each.</p> <p>In the first terminal window, execute the following command to start the application (both microservices).</p> <pre><code>docker compose -f docker/docker-compose-with-rdflib.yml up\nor\ndocker compose -f docker/docker-compose-with-jena.yml up\n</code></pre> <p>You should see similar verbose output that includes the following:</p> <p> </p> <p>In the second terminal window, execute the following command to terminate the application.</p> <pre><code>docker compose -f docker/docker-compose-with-rdflib.yml down\nor\ndocker compose -f docker/docker-compose-with-jena.yml down\n</code></pre> <p>You should see similar verbose output that includes the following:</p> <p> </p>"},{"location":"local_execution/#the-docker-containers","title":"The Docker Containers","text":"<p>These three pre-built Docker containers exist on DockerHub:</p> <ul> <li>cjoakim/caig_web_v2:latest</li> <li>cjoakim/caig_graph_v2:latest</li> <li>cjoakim/caig_graph_java_jena_v1:latest</li> </ul> <p>These are used by default by the above docker-compose scripts and also by the Azure Container App deployment process.</p> <p>If you wish to rebuild these containers and deploy them to your own Container Registry, please see the following Dockerfiles in this repo. You're free to modify these as necessary. Please change the cjoakim prefix to your own identifier.</p> <ul> <li>impl/app/docker/Dockerfile_graph</li> <li>impl/app/docker/Dockerfile_web</li> <li>impl/java_jena_graph_websvc/Dockerfile</li> </ul>"},{"location":"readme/","title":"Readme","text":"<p>The CosmosAIGraph (caig) application is deployed as these two microservices:</p> Name Functionality web Web Application microservice, user-facing, HTML oriented graph Graph Microservice, JSON microservices over an in-memory rdflib graph <p>These are located in the app_web, and app_graph directories of this repository.</p> <p>These are Docker-containerized with the caig_ prefix.  Therefore, the container names will be caig_web, and caig_graph.</p>"},{"location":"readme/#implementation-summary","title":"Implementation Summary","text":"<ul> <li>Python3 is used exclusively in the CosmosAIGraph solution</li> <li>See https://www.python.org</li> <li>FastAPI is used exclusively as the framework for the web and http services</li> <li>See https://fastapi.tiangolo.com</li> <li>Azure Cosmos DB is used as the persistent datastore for source data and session history as well as a vector database</li> <li>One or more Cosmos DB APIs may be part of your solution</li> <li>See https://learn.microsoft.com/en-us/azure/cosmos-db/</li> <li>You can use either the Cosmos DB for MongoDB vCore or Cosmos DB for NoSQL<ul> <li>https://learn.microsoft.com/en-us/azure/cosmos-db/mongodb/vcore/</li> </ul> </li> <li>Your domain data, with embeddings, are stored here</li> <li>AI sessions, prompt history, feedback, and completion history is persisted here</li> <li>This Cosmos DB data can optionally be mirrored to OneLake in Microsoft Fabric</li> <li>Azure OpenAI is used for AI and LLM functionality</li> <li>See https://learn.microsoft.com/en-us/azure/ai-services/openai/</li> <li>semantic-kernel is use for AI and LLM orchestration</li> <li>See https://learn.microsoft.com/en-us/semantic-kernel/overview/</li> <li>rdflib is used as the high-performance in-memory graph</li> <li>See https://rdflib.readthedocs.io/en/stable/</li> <li>SPARQL 1.1 is the graph query language</li> <li>See https://www.w3.org/TR/sparql11-query/</li> <li>Web Ontology Language (OWL) is the graph schema/ontology definition language</li> <li>See https://www.w3.org/OWL/</li> </ul>"},{"location":"readme/#quick-start","title":"Quick Start","text":""},{"location":"readme/#clone-this-github-repository","title":"Clone this GitHub Repository","text":"<p>Open a PowerShell Terminal, navigate to the desired parent directory and execute the following git clone command.  This will copy the contents of the public GitHub repository to your workstation.</p> <p>If you don't have git installed on your system, please see the Developer Workstation Setup page.</p> <pre><code>&gt; git clone https://github.com/cjoakim/CosmosAIGraph.git\n\n&gt; cd CosmosAIGraph\n\n&gt; Get-Location\n</code></pre> <p>The output value from the Get-Location will be a fully-qualified directory path on your workstation.  Please set the CAIG_HOME environment variable to this directory path value.</p> <pre><code>echo 'setting CAIG_HOME'\n[Environment]::SetEnvironmentVariable(\"CAIG_HOME\", \"...your value from Get-Location ...\", \"User\")\n</code></pre> <p>You will need to restart your Terminal for the above command to take effect.</p> <p>You'll see in a section below that this CosmosAIGraph reference application uses several environment variables, and they all begin with CAIG_.</p>"},{"location":"readme/#provision-azure-cosmos-db-and-azure-openai","title":"Provision Azure Cosmos DB and Azure OpenAI","text":"<ul> <li>See Initial PaaS Provisioning</li> </ul>"},{"location":"readme/#developer-workstation-setup","title":"Developer Workstation Setup","text":"<ul> <li> <p>See Developer Workstation Setup</p> </li> <li> <p>See Environment Variables</p> </li> </ul>"},{"location":"readme/#load-cosmos-db-with-library-and-config-documents","title":"Load Cosmos DB with Library and Config Documents","text":"<ul> <li>See Cosmos DB Document Design and Modeling</li> <li>See Load Azure Cosmos DB for MongoDB vCore</li> <li>See Load Azure Cosmos DB for NoSQL</li> </ul>"},{"location":"readme/#run-the-application-on-your-workstation","title":"Run the Application on your Workstation","text":"<ul> <li> <p>See Local Execution</p> </li> <li> <p>See Explore the FastAPI Framework and Endpoint Documentation</p> </li> <li> <p>See Understanding the Code</p> </li> </ul>"},{"location":"readme/#azure-container-app-deployment","title":"Azure Container App Deployment","text":"<ul> <li>See Deploying the Azure Container App</li> </ul>"},{"location":"readme/#screen-shots-of-the-current-implementation","title":"Screen Shots of the Current Implementation","text":"<ul> <li>See Screen Shots</li> </ul>"},{"location":"readme/#next-steps-customizing-this-solution-for-your-application","title":"Next Steps: Customizing this Solution for Your Application","text":"<p>It is recommended that CosmosAIGraph Proof-of-Concept (POC) team has the following skillsets:</p> <ul> <li>A data analyst who is familiar with your input graph data</li> <li>A data engineer who can wrangle/transform the raw data into JSON documents for Cosmos DB</li> <li> <p>A Python developer with UI skills</p> </li> <li> <p>See Customizing this Solution</p> </li> <li> <p>See Code Generation</p> </li> <li> <p>See the FAQ Page to clarify your understanding of the CosmosAIGraph solution.</p> </li> </ul>"},{"location":"screen_shots/","title":"CosmosAIGraph : Screen Shots","text":""},{"location":"screen_shots/#home-page","title":"Home Page","text":""},{"location":"screen_shots/#about-page","title":"About Page","text":""},{"location":"screen_shots/#sparql-console-page","title":"Sparql Console Page","text":"<p>A SPARQL query and its' results.</p> <p> </p> <p>A bill-of-material query of the Python Flask library.</p> <p> </p> <p>D3.js visualization of the Python Flask library bill-of-material graph.</p> <p> </p>"},{"location":"screen_shots/#generate-sparql-console-page","title":"Generate Sparql Console Page","text":"<p>A working SPARQL query generated by Azure Open AI from a  given OWL Ontology and user-specified natural language.</p> <p> </p>"},{"location":"screen_shots/#vector-search-console-page","title":"Vector Search Console Page","text":"<p>Vector search using the already existing embeddings for a given library.</p> <p> </p> <p>Vector search using natural language that is vectorized, then passed to the vector search.</p> <p> </p>"},{"location":"screen_shots/#azure-container-app-in-azure-portal","title":"Azure Container App in Azure Portal","text":""},{"location":"understanding_the_code/","title":"CosmosAIGraph : Understanding the Code","text":"<p>This page strives to guide the reader in understanding both the structure and implementation of this codebase.  Not every python module is described here, only the most important ones.</p> <p>Again, this is simply a reference application that you may wish to refer to as you design and implement your own solutions. Please feel free to use any part of this design and implementation, or none.</p>"},{"location":"understanding_the_code/#the-implapp-directory","title":"The impl\\app directory","text":"<p>This directory contains the Python codebase.</p> <p>It contains these directories:</p> <pre><code>\u251c\u2500\u2500 app               Application code and scripts\n\u251c\u2500\u2500 data              Data files, including the set of Python libraries\n\u251c\u2500\u2500 deployment        Bicep-based Azure Container App deployment scripts\n\u2514\u2500\u2500 docs              Documentation\n</code></pre>"},{"location":"understanding_the_code/#the-implapp-directory_1","title":"The impl\\app directory","text":"<p>This is where the Python implementation code and scripts are. It contains these directories.</p> <pre><code>\u251c\u2500\u2500 docker              Dockerfiles and docker-compose yml files\n\u251c\u2500\u2500 keys                Future use\n\u251c\u2500\u2500 ontologies          OWL schema files, with *.owl file suffix\n\u251c\u2500\u2500 rdf                 RDF graph data files in \"triples\" format, with *.nt file suffix\n\u251c\u2500\u2500 sparql              Sample SPARQL RDF query statements and Jinja2 templates\n\u251c\u2500\u2500 src                 Python source code\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 models\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 services\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 util\n\u251c\u2500\u2500 static              Static files and images served by the web application\n\u251c\u2500\u2500 templates           Jinja2 templates for non-web use\n\u251c\u2500\u2500 tests               pytest unit tests\n\u251c\u2500\u2500 tmp                 Create this directory manually; it should be git-ignored\n\u251c\u2500\u2500 venv                Python virtual environment directory, created by venv.ps1 and venv.sh\n\u2514\u2500\u2500 views               Jinja2 templates used to render HTML pages in the web application\n</code></pre> <p>See the tests.ps1 and tests.sh unit testing scripts. Their output looks like the following:</p> <p> </p>"},{"location":"understanding_the_code/#implappsrcmodels-directory","title":"impl\\app\\src\\models directory","text":"<p>File webservice_models.py contains the several Pydantic models  that define both the request and response payloads for the web endpoints in this application.</p> <p>Likewise, internal_models.py implements Pydantic Models used to describe datastructures passed between classes in the application.</p> <p>Pydantic models are somewhat like Java Interfaces in that they define what an object should look like, but provide no implementation. The FastAPI uses Pydantic models to generate endpoint documentation, and IDEs also use them to provide type hints to the Developer.</p>"},{"location":"understanding_the_code/#implappsrcservices-directory","title":"impl\\app\\src\\services directory","text":"<p>The classes here define the business service logic used by the application.</p> <p>config_service.py implements class ConfigService that is used by the application for all configuration values.  \"Hard-coding\" of these values is generally not done in this codebase.  It is a goal of this reference implementation to expose as much functionality via configuration and environment variables as possible.  See this class for the complete list of environment variables used by this solution, they all begin with CAIG_.</p> <p>ConfigService uses environment variables and this is well-suited for Docker containerized applications that are deployed to environments such as  Azure Container Apps (ACA) and Azure Kubernetes Service (AKS).</p> <p>The python-dotenv python library is used in the codebase to optionally get environment variables via the .env file.  The .env should not be stored in your source-control system, as it may contain application secrets.</p> <p>See the source code of this module; it lists all of the possible environment variables used in the codebase.  They all begin with CAIG_.</p> <p>logging_level_service.py defines the logic for returning a standard-library logging level such as logging.DEBUG, logging.INFO, etc, per the environment variable CAIG_LOG_LEVEL.  This solution uses the logging standard library rather than print statements.</p> <p>ai_service.py implements class AiService for all Azure OpenAI and semantic-kernel logic.  You are certainly free to use other orchestrators, such as LangChain. We chose semantic-kernel because the SDK was simpler and more stable.</p> <p>The generate_sparql_from_user_prompt method in this module is where the Azure OpenAI service is invoked to generate a SPARQL query from a given OWL ontology and user prompt.</p> <p>A primary method of AiService is invoke_kernel which generates a LLM completion and adds it to a user conversation.  These are instances of classes AiCompletion and AiConversation, respectively.</p> <p>AiConversation, in ai_conversation.py stores the conversational state of a given user conversation and contains the complete set of prompts, completions  (with token usage), history, as well as any user feedback. These conversations are persisted as JSON in the conversations container in Cosmos DB, and thus can be used for analysis of the performance of your AI application.</p> <p>The app directory contains file conversation.py that can be used to execute a conversation, with list of user natural language, directly from a terminal command-line rather than from the web application UI. Modifying and using conversation.py may accelerated your development process.</p> <p>cache_service.py implements all (optional) caching of results such as SPARQL queries. Cosmos DB is used as the implementation of the cache.</p> <p>db_service.py implements class DBService, which is a wrapper over classes CosmosNoSQLService and CosmosVCoreService.  These classes implement CRUD operations over these databases. Class DBService inherits from class BaseDBService. Which one is used at runtime is determined by the CAIG_GRAPH_SOURCE_TYPE environment variable. If the value is \"cosmos_nosql\" then the Cosmos DB NoSQL API is used. If the value is \"cosmos_vcore\" then the Cosmos DB Mongo vCore API is used.</p> <p>Class EntitiesService also inherits from BaseDBService, and is used to read a configuration document which lists the entity names in your system.  These names are used in class StrategyBuilder to determine the intent of user natural-language.</p> <p>graph_builder.py implements class GraphBuilder which utilizes the builder pattern to create an instance of class GraphService, which contains an in-memory rdflib database.  The source data for building the graph are JSON documents in Cosmos DB.  One Cosmos DB document may result in several triples being loaded into the in-memory graph.</p> <p>Alternatively, for dev/test environments, GraphBuilder can instantiate the in-memory graph by reading a RDF *.nt text file.</p> <p>These environment variables are used in building the in-memory rdflib graph: - CAIG_GRAPH_SOURCE_TYPE - CAIG_GRAPH_SOURCE_OWL_FILENAME - CAIG_GRAPH_SOURCE_RDF_FILENAME - CAIG_GRAPH_SOURCE_DB - CAIG_GRAPH_SOURCE_CONTAINER - CAIG_AZURE_MONGO_VCORE_CONN_STR </p> <p>Please see the Environment Variables page where these are described.</p> <p>Please take a close look at graph_builder.py to see how the in-memory rdflib graph is populated.  One approach uses an RDF triples file (i.e. - .nt) while the other loads the graph from Cosmos DB* documents.</p> <p>Note that code generation may be used to generate  a separate class named RdflibTriplesBuilder that is used by class GraphBuilder in graph_builder.py to actually load the rdflib graph.</p> <p>graph_service.py implements class GraphService which provides an interface to all rdflib functionality including SPARQL queries.  The in-memory rdflib graph is mutable.  Since it resides in-memory, rather than on disk, it results in low-latency queries.</p> <p>entities_service.py implements class EntitiesService which allows you to define the names of the entities in your system, and thus identify them in user natural-language.</p> <p>strategy_builder.py implements class StrategyBuilder used to determine the strategy to be used in obtaining RAG data.  The currently identified strategies are: db, graph, and vector.  This class uses EntitiesService.</p> <p>rag_data_service.py implements class RAGDataService which is used to fetch the pertinent RAG data given a user utterance or natural-language. This class uses StrategyBuilder to determine the appropriate strategy to use when fetching the RAG data.  The RAG data can be read directly from  Cosmos DB if the stragegy is \"db\", otherwise either an in-memory RDF graph query or a Cosmos DB vector search is used to identify and fetch the RAG data.</p> <p>rag_data_result.py implements class RAGDataResult.  Instances of this class are returned by the get_rag_data method of the above class RAGDataService.</p>"},{"location":"understanding_the_code/#implappsrcutil-directory","title":"impl\\app\\src\\util directory","text":"<p>fs.py provides methods for accessing the local filesystem.</p> <p>owl_formatter.py is used to return a minimized version of a given OWL file content in order to be passed efficiently in an AzureOpenAI system prompt.</p> <p>prompts.py is used to define system and user prompts for  Azure OpenAI.  This module is expected to evolve as the codebase adds more semantic-kernel functionality.</p> <p>prompt_optimizer.py implements class PromptOptimizer which is used in class AiService to optimize/truncate the conversation context and history so as to fit within a given Azure OpenAI max_tokens value.</p> <p>PromptOptimizer also generates a high-fidelity prompt text value that can  be stored in the AiConversation and displayed in the UI. Both semantic-kernel and LangChain seem to do a poor job of exposing the actual merged prompt text used with invoking the LLM.</p> <p>sparql_formatter.py is used to return 'pretty' version of a given (AI-generated) SPARQL query.</p> <p>sparql_template.py is used render dynamic SPARQL queries from jinja2 templates and a dictionary of values.</p> <p>owl_sax_handler.py implements class OwlSaxHandler which can be used to parse your ontology XML file via the SAX API.  The parsed output can be used to produce a visualization of your ontology, such as with D3.js.</p>"},{"location":"understanding_the_code/#the-fastapi-entry-point-modules","title":"The FastAPI entry-point modules","text":"<p>The entry-point for the two microservices are:</p> <ul> <li>impl/app/webapp.py    This is the Web Application</li> <li>impl/app/websvc.py    This is the Graph Microservice</li> </ul> <p>These two are very similar in that they create the FastAPI object, load environment variables, configure logging, log the environment variables, and define the HTTP endpoints.</p> <p>The following is an example from the AI microservice.</p> <pre><code># standard initialization\nload_dotenv(override=True)\nlogging.basicConfig(\n    format=\"%(asctime)s - %(message)s\", level=LoggingLevelService.get_level()\n)\nConfigService.log_defined_env_vars()\n\napp = FastAPI()\nai_svc = AiService()\n</code></pre>"},{"location":"understanding_the_code/#liveness-endpoints","title":"/liveness endpoints","text":"<p>Each microservice implements a /liveness HTTP GET endpoint that can be executed by the container orchestration system (i.e - ACA or AKS) to ensure the process is alive, and restart it if not.</p>"},{"location":"understanding_the_code/#authorization-and-authentication","title":"Authorization and Authentication","text":"<p>Since each customer organization is expected to have their own specific security requirements, Authorization and Authentication is not implemented in this reference application.</p> <p>msal is one library you may wish to use to implement authorization and authencication with Microsoft Entra (AAD).</p> <p>On a networking level, however, the Azure Container App will user use its' own, or an injected, Virtual network.  So the back-end microservices can be defined as \"internal\" and not receive external network requests.</p> <p>Additionally, middleware can be used in the Graph Microservice such that a known HTTP header must be populated with a known value before the HTTP request is accepted and processed.  Otherwise, a HTTP 401 return code is returned.  See \"@app.middleware(\"http\")\" in file app\\websvc.py.</p>"},{"location":"understanding_the_code/#code-formatting-with-the-black-library","title":"Code formatting with the black library","text":"<p>The black library is used to reformat all python source code (i.e - the .py files) into a standard and pythonic* style.  This can eliminate friction in Development teams by eliminating personal coding style preferences.  In addition to reformatting the source code, black can also identify some problems/errors in the code.</p> <p>See the code-reformat.ps1 and code-reformat.sh scripts in the impl directory.</p>"},{"location":"understanding_the_code/#the-impljava_jena_graph_websvc-directory","title":"The impl\\java_jena_graph_websvc directory","text":"<p>This directory contains the newer Graph Microservice implemented with Java, Spring Boot, and Apache Jena.  Gradle is used as the build tool.</p> <p>Please see the readme.md file in this directory regarding building and executing this version of the Graph Microservice.</p> <p>It contains these directories:</p> <pre><code>\u251c\u2500\u2500 build                Output of the Gradle-based compilation and packaging process\n\u251c\u2500\u2500 data\n\u251c\u2500\u2500 data/cosmosdb_documents.json   \n\u251c\u2500\u2500 ontologies\n\u251c\u2500\u2500 rdf                  RDF files optionally used to load the graph\n\u251c\u2500\u2500 src                  Standard Java source code directory structure\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 main\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 test\n\u2514\u2500\u2500 tmp                  Create this directory if it doesn't already exist\n</code></pre>"},{"location":"understanding_the_code/#key-classes-in-the-javajena-implementation","title":"Key Classes in the Java/Jena implementation","text":"<p>This section describes the primary Java classes in the Java/Jena implementation.</p>"},{"location":"understanding_the_code/#commicrosoftcosmosdbcaigwebapp","title":"com.microsoft.cosmosdb.caig.WebApp","text":"<p>This is the Spring Boot entry point for the application, per the  @SpringBootApplication annotation.</p>"},{"location":"understanding_the_code/#commicrosoftcosmosdbcaigwebgraphrestcontroller","title":"com.microsoft.cosmosdb.caig.web.GraphRestController","text":"<p>Spring @RestController that implements the /sparql_query HTTP endpoint. This endpoint is invoked by the Python-based Web UI when executing graph queries.</p>"},{"location":"understanding_the_code/#commicrosoftcosmosdbcaigwebhealthrestcontroller","title":"com.microsoft.cosmosdb.caig.web.HealthRestController","text":"<p>Spring @RestController that implements the /health HTTP endpoint. This endpoint that can optionally be invoked by your container orchestrator runtime environment - such as Azure Container Apps (ACA) or Azure Kubernetes Service (AKS).</p>"},{"location":"understanding_the_code/#commicrosoftcosmosdbcaigwebpingrestcontroller","title":"com.microsoft.cosmosdb.caig.web.PingRestController","text":"<p>Spring @RestController that implements the / and /ping HTTP endpoints. The / endpoint simply returns the epoch time, while /ping returns uptime and JVM memory information.</p>"},{"location":"understanding_the_code/#commicrosoftcosmosdbcaigwebappstartup","title":"com.microsoft.cosmosdb.caig.web.AppStartup","text":"<p>This contains the application startup logic per the Spring ApplicationListener interface.  It contains this logic which  loads the in-memory graph in class AppGraph.</p> <pre><code>  AppGraph g = AppGraphBuilder.build(null);\n  AppGraph.setSingleton(g);```\n</code></pre>"},{"location":"understanding_the_code/#commicrosoftcosmosdbcaigutilappconfig","title":"com.microsoft.cosmosdb.caig.util.AppConfig","text":"<p>This static class returns almost all configuration values for the  application, such as from environment variables.</p> <p>The environment variables begin with the CAIG_ prefix and are described in the Environment Variables page</p> <p>The Spring Boot framework also uses the src/main/resources/application.properties file for some configuration values.  But all application coniguration is done with environment variables and class AppConfig.  This approach  is typically used by Docker containerized applications.</p>"},{"location":"understanding_the_code/#commicrosoftcosmosdbcaiggraphappgraphbuilder","title":"com.microsoft.cosmosdb.caig.graph.AppGraphBuilder","text":"<p>This class creates and populates the in-memory graph object which is an instance of class org.apache.jena.rdf.model.Model.</p> <p>AppGraphBuilder can populate the graph from one of several  sources per the CAIG_GRAPH_SOURCE_TYPE environment variable. CAIG_GRAPH_SOURCE_TYPE may have one of the following values:</p> <ul> <li>json_docs_file - the graph is sourced from ile cosmosdb_documents.json in the repo</li> <li>rdf_file - the graph is sourced from the file specified in CAIG_GRAPH_SOURCE_RDF_FILENAME</li> <li>cosmos_nosql - the graph is sourced from your Cosmos DB NoSQL account</li> <li>cosmos_vcore - the graph is sourced from your Cosmos DB Mongo vCore account</li> </ul> <p>After the Jena graph is populated, you can optionally dump that graph to a file per the CAIG_GRAPH_DUMP_UPON_BUILD and CAIG_GRAPH_DUMP_OUTFILE environment variables.</p>"},{"location":"understanding_the_code/#commicrosoftcosmosdbcaiggraphappgraph","title":"com.microsoft.cosmosdb.caig.graph.AppGraph","text":"<p>This class contains the singleton instance of class org.apache.jena.rdf.model.Model in the Apache Jena SDK.</p> <p>It implements this primary message signature:</p> <pre><code>    public synchronized SparqlQueryResponse query(SparqlQueryRequest request) {\n    }\n</code></pre> <p>The method is synchronized so as to be thread safe.  Each HTTP request  to the Spring Boot application runs in its' own Thread.</p> <p>Classes SparqlQueryRequest and SparqlQueryResponse are simple JSON serializable classes used to receive the HTTP POSTed query and to return the JSON response to the query. </p>"},{"location":"understanding_the_code/#commicrosoftcosmosdbcaiggraphlibrariesgraphtriplesbuilder","title":"com.microsoft.cosmosdb.caig.graph.LibrariesGraphTriplesBuilder","text":"<p>For the cases where AppGraphBuilder sources the graph from Cosmos DB, instances of LibrariesGraphTriplesBuilder are uses to populate the graph from each appropriate Cosmos DB document.</p> <p>Customers should implement their own GraphTriplesBuilder class for their needs and implement as necessary per the shape of your Cosmos DB documents and graph schema.</p>"}]}